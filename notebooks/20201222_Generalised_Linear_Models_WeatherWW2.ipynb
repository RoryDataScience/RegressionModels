{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Start Date** <br>\n",
    "17th December 2020 <br>\n",
    "\n",
    "**Data Sources** <br>\n",
    "https://www.kaggle.com/usaf/world-war-ii/notebooks <br> \n",
    "https://www.kaggle.com/smid80/weatherww2/data <br>\n",
    "\n",
    "**Background** <br>\n",
    "Aerial Bombing Operations in WW2 - Bombing operations data <br>\n",
    "\n",
    "This dataset consists of digitized paper mission reports from WWII. Each record includes the date, conflict, geographic location, and other data elements to form a live-action sequence of air warfare from 1939 to 1945. The records include U.S. and Royal Air Force data, in addition to some Australian, New Zealand and South African air force missions.\n",
    "\n",
    "Weather Conditions in WW2 (Weather Stations / Weather Conditions) <br>\n",
    "The dataset contains information on weather conditions recorded on each day at various weather stations around the world. Information includes precipitation, snowfall, temperatures, wind speed and whether the day included thunder storms or other poor weather conditions.\n",
    "\n",
    "**Aim of this project** <br>\n",
    "Use Sweetviz - A powerful package to speed up EDA, saving reports as HTML files and with the ability to compare test and train datasets \n",
    "Impletment the use of Ridge Plots as part of EDA to understand variabe distribution analysis \n",
    "One method of Outlier Detection & Analysis\n",
    "Implement a GLM model that predicts the maximum weather temperature (based on the minimum temperature)\n",
    "Test Assumptions of GLMs & Residual Analysis: Correlation of errors with predictor variables\n",
    "Dockerise the notebook\n",
    "\n",
    "\n",
    "**Analysis regarding Data Quality** <br>\n",
    "Understanding of the sampling procedure \n",
    "- Since our project team did not participate in planning the study or data collection, it is possible that we are missing crucial context which could render our conclusions invalid. <br>\n",
    "\n",
    "Potential biases <br> \n",
    "Real-world actions that generated the data you inherited <br>\n",
    "\n",
    "**Objectives & Hypothesises to Test (max. 10)** <br>\n",
    "<u>Exploratory Analysis</u>\n",
    "- High level discriptive statistics \n",
    "- Do any values look to be recorded to accommodate missing values? e.g. 999, 9999 etc.\n",
    "- Assessment of feature distributions\n",
    "- Assessment of feature relationships:\n",
    "    - What defines the feature 'poor weather' conditions?\n",
    "    - Is there a relationship between the daily minimum and maximum temperature (TimeSeries Analysis)?\n",
    "    - It is expected that average temperatures are colder in winter months than summer months\n",
    "    - It is expected that more snowfall occurs in the winter months (for northern hemisphere regions)\n",
    "    - It is expected that more Precipitation occurs in the winter months (for northern hemisphere)\n",
    "    - It is expected that lower temperatures correlate with higher snowfall and precipation \n",
    "    - It is expected that higher levels above the sea have greater precipation\n",
    "    - It is expected that the accuracy of recordings based on stations may not be uniform (outlier detection)\n",
    "<br>\n",
    "\n",
    "**Statistical Model/Machine Learning Applications**\n",
    "- Create a dummy model (Predict the average temperature for that monthly/quarter)\n",
    "- Explain the train/test split\n",
    "- Predict the maximum temperature given the minimum temperature (GLM Models & Bayesian Versions)?\n",
    "- Explain appropriate error metric\n",
    "- Explain class balance and any required action\n",
    "- Explain what features are developed and transformations applied\n",
    "- Explain if the model is exhibiting high bias or high variance and how this can be improved\n",
    "    - Plot learning curves to deduce high bias/high variance and conclude what means could be applied to solve these issues\n",
    "- Explain where the model seems to perform poorly - In what situations does the model make mistakes?\n",
    "\n",
    "**Additional Learning notes from Reviewing 3 other Kaggle Notebooks** <br> \n",
    "\n",
    "**Next steps** <br>\n",
    "\n",
    "**References**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Package Requirements\n",
    "import os\n",
    "import sys\n",
    "# !{sys.executable} -m pip install markdown\n",
    "# !{sys.executable} -m pip install sweetviz\n",
    "# !{sys.executable} -m pip install joypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "\n",
    "# Data Exploration and Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sweetviz as sv\n",
    "import joypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aerial_bombing_data = pd.read_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/01_raw/ww2_boming_operations.csv')\n",
    "weather_summary = pd.read_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/01_raw/WeatherTempPrediction.csv')\n",
    "weather_station_location = pd.read_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/01_raw/WeatherStationLocations.csv')\n",
    "\n",
    "data_list = []\n",
    "data_list.append(aerial_bombing_data)\n",
    "data_list.append(weather_summary)\n",
    "data_list.append(weather_station_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State the assumptions you’re being forced to make.\n",
    "# Write up caveat notes to be included in the appendix of your final report\n",
    "# Write cautionary notes that warn the decision-maker (and your other readers) that conclusions from the study will \n",
    "# need to be downgraded due to potential data issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_list:\n",
    "    print(\"Dataframe Dimensions\")\n",
    "    print(i.shape)\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Dataframe Columns and respective types\")\n",
    "    print(i.dtypes)\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Percentage of Missing Data\")\n",
    "    print(i.isnull().sum() * 100 / len(i))\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investiage options to link the dataframes with a unique key: weather_summary and weather_station_location look to be \n",
    "# connected via STA and WBAN respectively \n",
    "def uncommon_elements(list1, list2):\n",
    "    ## Add something clever so the look up is always against the set with the largest number of unique records\n",
    "    \n",
    "    return [element for element in list2 if element not in list1]\n",
    "\n",
    "STA = set(weather_summary.STA)\n",
    "print(len(STA))\n",
    "\n",
    "WBAN = set(weather_station_location.WBAN)\n",
    "print(len(WBAN))\n",
    "\n",
    "print('Sets are of the same data type: ', type(weather_summary.STA) == type(weather_station_location.WBAN))\n",
    "\n",
    "print('Stations that are uncommon across both sets: ', uncommon_elements(STA, WBAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.merge(weather_summary, \n",
    "                         weather_station_location, \n",
    "                         how = 'inner', # takes care of only keeping records in both sets\n",
    "                         left_on='STA',\n",
    "                         right_on='WBAN')\n",
    "\n",
    "print(len(combined_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns for Combined Data\n",
    "**STA**\n",
    "- STA: represent the Weather Station\n",
    "- Not all STA codes represent the same time frequency \n",
    "- Object Data Type\n",
    "\n",
    "**Date** \n",
    "- Date has been split into DA MO and YR respectively, note the century has been dropped when recording the YR\n",
    "- Date Data Type (will require engineering in order to be used for ML\n",
    "\n",
    "**Precip** \n",
    "- Precipitation in mm. This consists of numerical values and 'T' for 16,754 entries. This looks to be a mistake in the data collection (Impute precip == 0 for these cases)\n",
    "- Numeric (float) Data Type\n",
    "\n",
    "**MaxTemp and MinTemp** \n",
    "- These are features that have been transformed into celcius from fahrenheit readings MAX/MIN and these have been recorded to 6 decimal places. The degrees celcius value has additionally been converted to an average. Using celcius will have a smaller range than the fahrenheit records. Patterns may be more easily seen based fahrenheit columns \n",
    "- Numeric (float) Data Type\n",
    "\n",
    "**MEA** \n",
    "- This is the mean for the fahrenheit MAX / MIN columns and this has been rounded to 1 d.p. Drop this columns and calculate the extact mean value\n",
    "- Numeric (float) Data Type\n",
    "\n",
    "**Snowfall**\n",
    "- This looks to be measures in terms of the amount of snow that fell in mm. The units are not obvious so there are two options\n",
    "- Either assume the units are centiments by attempting to research more about the data OR normalise all the columns so they are on the same scale\n",
    "- Numeric (float) Data Type\n",
    "\n",
    "**SNF**\n",
    "- After research it is unclear what SNF relates too and seems to gave a range of 0 - 3.4 (Agree to remove)\n",
    "- This supports the necessity to normalise the data for the numeric columns due to the possibility of differing units across columns\n",
    "- Numeric (float) Data Type\n",
    "\n",
    "**PRCP**\n",
    "- This column looks to have been scaled by a factor of 1/25.4*Precip (Agree to remove)\n",
    "- Numeric (float) Data Type\n",
    "\n",
    "**TSHDSBRSGF**\n",
    "- This is a repeat for PoorWeather so can be removed\n",
    "\n",
    "**WBAN**\n",
    "- Same as STA, representing the Weather Station\n",
    "- Not all Weather Stations are located in the USA (unique STATE/COUNTRY ID = 63)\n",
    "- This will be duplicated due to the merge so can be removed \n",
    "- Object Data Type\n",
    "\n",
    "**NAME**\n",
    "- This is the name of the weather station. It has a many:1 relationship with State/Country ID i.e. more than one station can be present per country \n",
    "- Object Data Type (Nominal)\n",
    "\n",
    "**STATE/COUNTRY ID**\n",
    "- This is the location of the weather station at state/country level\n",
    "- Object Data Type (Nominal)\n",
    "\n",
    "**LAT**\n",
    "- This is the decimal latitude in string format \n",
    "\n",
    "**LON**\n",
    "- This is the decimal latitude in string format \n",
    "\n",
    "**ELEV**\n",
    "- Explanation not given - Expected to be level above the sea \n",
    "- Note that an elevation of 9999 means unknown\n",
    "- Numeric (float) Data Type\n",
    "\n",
    "**Latitude**\n",
    "- This is the decimal latitude calculated from the LAT/LON provided (use this over string as in format for ML)\n",
    "\n",
    "**Longitude**\n",
    "- This is the decimal longitude calculated from the LAT/LON provided (use this over string as in format for ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - Remove duplicate Rows & Columns\n",
    "- Remove all columns that exhibit over 90% missing values\n",
    "- Remove celcius columns 'MaxTemp', 'MinTemp', 'MeanTemp' and 'MEA'\n",
    "- Remove duplicated/scaled columns: 'PRCP', 'TSHDSBRSGF'\n",
    "- Remove PoorWeather for the inital analysis as unclear how the data has been recorded \n",
    "- Remove the primary key to join dataframes SNF & WBAN\n",
    "- Remove LAT as string format\n",
    "- Remove LON as string format\n",
    "- Remove those columns with zero variance\n",
    "- Remove duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing data - Remove any columns with over 90% missing data \n",
    "def remove_missing_values(data, thresold_limit = 0.9):\n",
    "    \n",
    "    return data.loc[:, data.isnull().sum() < thresold_limit*data.shape[0]]\n",
    "\n",
    "combined_data = remove_missing_values(combined_data)\n",
    "\n",
    "# Remove additional columns based on explanation above\n",
    "combined_data.drop(['MaxTemp', \n",
    "                    'MinTemp', \n",
    "                    'MeanTemp', \n",
    "                    'MEA', \n",
    "                    'TSHDSBRSGF', \n",
    "                    'PRCP', \n",
    "                    #'PoorWeather', \n",
    "                    'SNF', \n",
    "                    'WBAN',\n",
    "                    'LAT', \n",
    "                    'LON'], axis=1, inplace=True)\n",
    "\n",
    "# Data Quality Expectations: Test for zero variance \n",
    "combined_data = combined_data.loc[:, combined_data.apply(pd.Series.nunique) != 1]\n",
    "\n",
    "# Data Quality Expectations: Duplicated Records\n",
    "print('Duplicated rows for index: ', combined_data[combined_data.duplicated()].index)\n",
    "#print(len(combined_data))\n",
    "combined_data = combined_data.drop_duplicates()\n",
    "#print(len(combined_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- Understand and update data to correct type [x]\n",
    "- Check that variables are within a range expected \n",
    "- Assessment of categorical labels and confirm they are as expected\n",
    "- Evaluate where missing data exists and how to deal with these fields\n",
    "- Data represents logical coherence (e.g. underaged cannot hold a driving licence)\n",
    "- Reformatting: Drop/Rename columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Date\n",
    "def correct_dates(data, cols):\n",
    "    \n",
    "    return pd.to_datetime(combined_data[cols], format = '%Y-%m-%d')\n",
    "\n",
    "# Correct Object Types\n",
    "def correct_objects(data, cols):\n",
    "    \n",
    "    return data[cols].astype('object')\n",
    "\n",
    "# Correct String Types\n",
    "def correct_string(data, cols):\n",
    "    \n",
    "    return data[cols].astype(str)\n",
    "\n",
    "combined_data.Date = correct_dates(combined_data, 'Date')\n",
    "\n",
    "combined_data.STA = correct_objects(combined_data, 'STA')\n",
    "combined_data.YR = correct_objects(combined_data, 'YR')\n",
    "combined_data.MO = correct_objects(combined_data, 'MO')\n",
    "combined_data.DA = correct_objects(combined_data, 'DA')\n",
    "\n",
    "combined_data.Snowfall = correct_string(combined_data, 'Snowfall')\n",
    "combined_data.PoorWeather = correct_string(combined_data, 'PoorWeather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_report = sv.analyze(combined_data)\n",
    "my_report.show_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with Missing/Inaccurate values and correct data types \n",
    "def impute_missing_values(data, cols):\n",
    "    \n",
    "    return np.where((data[cols] == 'T') | (data[cols] == ' '), 0, data[cols])\n",
    "\n",
    "# Correct float types\n",
    "def correct_floats(data, cols):\n",
    "    \n",
    "    return data[cols].astype('float')\n",
    "\n",
    "# As the target variable contains 4% missing values remove these to avoid inaccurate assumptions\n",
    "def remove_missing_max(data):\n",
    "    \n",
    "    return data[data.loc[:, 'MAX'].notnull()]\n",
    "\n",
    "def remove_missing_min(data):\n",
    "    \n",
    "    return data[data.loc[:, 'MIN'].notnull()]\n",
    "\n",
    "combined_data.Precip = impute_missing_values(combined_data, 'Precip')\n",
    "combined_data.Precip = correct_floats(combined_data, 'Precip')\n",
    "\n",
    "combined_data = remove_missing_max(combined_data)\n",
    "combined_data = remove_missing_min(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the above logic has been correctly applied \n",
    "print(combined_data[combined_data.loc[:, 'MAX'].isnull()])\n",
    "print(combined_data[combined_data.loc[:, 'MIN'].isnull()])\n",
    "print((combined_data.isnull().sum() * 100) / len(combined_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Combination Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = combined_data.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling nominal and ordinal categorical values\n",
    "Types: \n",
    "- Ordinal: Convert into numeric values retaining the datas natural order\n",
    "- Nominal: One Hot encoding/Label Encoding\n",
    "- Dichotomous (Binary): Convert values into indicator values 1/0 <br>\n",
    "The only columns that require revision at this stage are 'NAME' and 'STATE/COUNTRY ID'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_category_datatype(data, cols):\n",
    "    \n",
    "    return data[cols].astype('category')\n",
    "\n",
    "combined_data.NAME = correct_category_datatype(combined_data, 'NAME')\n",
    "combined_data['STATE/COUNTRY ID'] = correct_category_datatype(combined_data, 'STATE/COUNTRY ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas `.get_dummies()` method\n",
    "NAME_dummies_df = pd.concat([combined_data, pd.get_dummies(combined_data['NAME'], prefix='category')],axis=1)\n",
    "STATE_dummies_df = pd.concat([combined_data, pd.get_dummies(combined_data['STATE/COUNTRY ID'], prefix='category')],axis=1)\n",
    "\n",
    "# Now drop the original 'category' column (you don't need it anymore)\n",
    "NAME_dummies_df.drop(['NAME'],axis=1, inplace=True)\n",
    "STATE_dummies_df.drop(['STATE/COUNTRY ID'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_dummies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "prefix = 'category'\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ohe = ohe.fit(combined_data[['category']])\n",
    "onehot_encoded = ohe.transform(combined_data[['category']])\n",
    "features_names_prefixed = [ f\"{prefix}_{category}\" for category in onehot_encoder.categories_[0]]\n",
    "category_enncoded_df = pd.concat([combined_data, pd.DataFrame(onehot_encoded, columns=features_names_prefixed)], axis=1)\n",
    "# now drop the original 'category' column (you don't need it anymore)\n",
    "category_enncoded_df.drop(['category'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis \n",
    "**Hypothesis & Expectations to Test**\n",
    "- Is this a Global study? What are the locations associated with the experiment? [x]\n",
    "- High level discriptive statistics [x]\n",
    "- Assessment of feature distributions [x]\n",
    "- Do any values look to be recorded to accommodate missing values? e.g. 999, 9999 etc.\n",
    "- Assessment of feature relationships:\n",
    "    - What is the relationship of each variable with the target?\n",
    "- Data Integrity Preprocessing Steps\n",
    "    - It is expected that average temperatures are colder in winter months than summer months\n",
    "    - It is expected that more snowfall occurs in the winter months (for northern hemisphere regions)\n",
    "    - It is expected that more Precipitation occurs in the winter months (for northern hemisphere)\n",
    "    - It is expected that lower temperatures correlate with higher snowfall and precipation \n",
    "    - It is expected that higher levels above the sea have greater precipation\n",
    "    - It is expected that the accuracy of recordings based on stations may not be uniform (outlier detection)\n",
    "- Create a function that differentiates between the Northern & Southern Hemisphere if using monthly data\n",
    "- Time Series Analysis (max. 3 graphs/analyses)\n",
    "\n",
    "**References** <br>\n",
    "https://towardsdatascience.com/powerful-eda-exploratory-data-analysis-in-just-two-lines-of-code-using-sweetviz-6c943d32f34 <br>\n",
    "https://medium.com/analytics-vidhya/how-to-plot-data-on-a-world-map-in-python-25cf9733c3dd <br>\n",
    "https://towardsdatascience.com/pandas-profiling-sweetviz-8849704cadd7 <br>\n",
    "https://towardsdatascience.com/ridgeline-plots-the-perfect-way-to-visualize-data-distributions-with-python-de99a5493052 <br>\n",
    "https://towardsdatascience.com/all-you-want-to-know-about-preprocessing-data-preparation-b6c2866071d4 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this a Global study? What are the locations associated with the experiment?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical data doesn’t have duplicates because of whitespaces, lower/upper cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['STATE/COUNTRY ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.NAME.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(combined_data['STATE/COUNTRY ID'], combined_data.NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High Level Descriptive Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assessment of feature distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density Plots for Numerical Columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['float64']).columns\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(7,7))\n",
    "\n",
    "for i in numeric_columns:\n",
    "    sns.distplot(combined_data[i], hist=True, kde=True, color = 'darkblue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = combined_data.select_dtypes(include=['object']).columns\n",
    "object_columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['Snowfall'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplots for Categorical Data\n",
    "object_columns = combined_data.select_dtypes(include=['object']).columns\n",
    "object_columns = object_columns[1:] #  Cannot plot STA\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(7,7))\n",
    "\n",
    "for i in object_columns:\n",
    "    combined_data[i].value_counts().plot(kind='bar')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(combined_data.YR, combined_data.MO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joypy import joyplot\n",
    "feature_data = combined_data[['Date', 'NAME', 'MIN', 'MAX']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data.NAME.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_data = feature_data.query(\"NAME == 'WHEELER/AFB 810.1'\")\n",
    "ridge_data = ridge_data.drop('NAME', axis=1)\n",
    "ridge_data['Date'] = ridge_data['Date'].astype('datetime64')\n",
    "ridge_data['Month'] = ridge_data['Date'].dt.month_name()\n",
    "\n",
    "ridge_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "cat_month = CategoricalDtype(\n",
    "    ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "     'July', 'August', 'September', 'October', 'November', 'December']\n",
    ")\n",
    "\n",
    "ridge_data['Month'] = ridge_data['Month'].astype(cat_month)\n",
    "\n",
    "ridge_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "joyplot(\n",
    "    data=ridge_data[['MAX', 'Month']], \n",
    "    by='Month',\n",
    "    figsize=(12, 8)\n",
    ")\n",
    "\n",
    "plt.title('Ridgeline Plot of Max Temperatures in WHEELER/AFB 810.1', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "ax, fig = joyplot(\n",
    "    data=ridge_data[['MIN', 'MAX', 'Month']], \n",
    "    by='Month',\n",
    "    column=['MIN', 'MAX'],\n",
    "    color=['#686de0', '#eb4d4b'],\n",
    "    legend=True,\n",
    "    alpha=0.85,\n",
    "    figsize=(12, 8)\n",
    ")\n",
    "plt.title('Ridgeline Plot of Min and Max Temperatures in WHEELER/AFB 810.1', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write clean data to 02_intermediate data folder\n",
    "#weather_summary.to_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/02_intermediate/data_cleaning.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - Feature Engineering\n",
    "- Deal with date columns. There are 4 most common methods to transform date to numeric format:\n",
    "    - Unix timestamp: Time since 1970 (not applicable as our data is from period 1940 - 1944\n",
    "    - KSP date formate: Year and quarter are obvious and attempts to retain similar intervals\n",
    "    - Divide into several features (DA / MO / YR): Data already formated to accommodate these features\n",
    "    - Manual Feature Creation: Time from or to an event\n",
    "- Feature Creation:\n",
    "    - Northern/Southern Hemisphere Flag\n",
    "    - Binary Flag for univariate outliers\n",
    "    - Create sensible bins for numerical Variables\n",
    "    - Sine & Cos features (out of scope)\n",
    "    - Average temperature per month/per quarter\n",
    "- Normalization and standardization of features: Precip/Snowfall/SNF/ELEV\n",
    "- Dimensionality Reduction <br>\n",
    "\n",
    "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159 <br>\n",
    "https://towardsdatascience.com/cyclical-features-encoding-its-about-time-ce23581845ca <br>\n",
    "https://medium.com/@khadijamahanga/using-latitude-and-longitude-data-in-my-machine-learning-problem-541e2651e08c <br>\n",
    "https://towardsdatascience.com/geopandas-101-plot-any-data-with-a-latitude-and-longitude-on-a-map-98e01944b972"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date Time Features** <br>\n",
    "Note: From there analysis it is clear that some areas from both the northern and southern hemisphere have been included as part of the research so care should be taken when dealing with datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import calendar\n",
    "\n",
    "def to_ksp_format(datetime):\n",
    "    year = datetime.year\n",
    "    day_from_jan_1 = (datetime - dt.datetime(year, 1, 1)).days\n",
    "    is_leap_year = int(calendar.isleap(year))\n",
    "    \n",
    "    return year + (day_from_jan_1 - 0.5) / (365 + is_leap_year)\n",
    "\n",
    "combined_data['ksp_date'] = combined_data['Date'].apply(to_ksp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arithmetic Features** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outlier Features** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geo-Spatial Data (Latitude & Longitude)**\n",
    "Converting geolocation data into zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average based on the fahrenheit columns\n",
    "weather_summary['MeanTemp_F'] = (weather_summary['MAX'] + weather_summary['MIN'])/2\n",
    "\n",
    "# Create a simplier binary feature for snowfall occurrence\n",
    "weather_summary['Snowfall_bin'] =  np.where(weather_summary.Snowfall.isnull(), 0, 1)\n",
    "\n",
    "# Add full state names to the analysis\n",
    "AMERICAN_STATES_TUPLE = [(\"AL\",\"Alabama\"),\n",
    "                         (\"AK\",\"Alaska\"),\n",
    "                         (\"AZ\",\"Arizona\"),\n",
    "                         (\"AR\",\"Arkansas\"),\n",
    "                         (\"CA\", \"California\"),\n",
    "                         (\"CO\", \"Colorado\"),\n",
    "                         (\"CT\",\"Connecticut\"),\n",
    "                         (\"DC\",\"Washington DC\"),\n",
    "                         (\"DE\",\"Delaware\"),\n",
    "                         (\"FL\",\"Florida\"),\n",
    "                         (\"GA\",\"Georgia\"),\n",
    "                         (\"HI\",\"Hawaii\"),\n",
    "                         (\"ID\",\"Idaho\"),\n",
    "                         (\"IL\",\"Illinois\"),\n",
    "                         (\"IN\",\"Indiana\"),\n",
    "                         (\"IA\",\"Iowa\"),\n",
    "                         (\"KS\",\"Kansas\"),\n",
    "                         (\"KY\",\"Kentucky\"),\n",
    "                         (\"LA\",\"Louisiana\"),\n",
    "                         (\"ME\",\"Maine\"),\n",
    "                         (\"MD\",\"Maryland\"),\n",
    "                         (\"MA\",\"Massachusetts\"),\n",
    "                         (\"MI\",\"Michigan\"),\n",
    "                         (\"MN\",\"Minnesota\"),\n",
    "                         (\"MS\",\"Mississippi\"),\n",
    "                (\"MO\",\"Missouri\"),\n",
    "                (\"MT\",\"Montana\"),\n",
    "                (\"NE\",\"Nebraska\"),\n",
    "                (\"NV\",\"Nevada\"),\n",
    "                (\"NH\",\"New Hampshire\"),\n",
    "                (\"NJ\",\"New Jersey\"),\n",
    "                (\"NM\",\"New Mexico\"),\n",
    "                (\"NY\",\"New York\"),\n",
    "                (\"NC\",\"North Carolina\"),\n",
    "                (\"ND\",\"North Dakota\"),\n",
    "                (\"OH\",\"Ohio\"),\n",
    "                (\"OK\",\"Oklahoma\"),\n",
    "                (\"OR\",\"Oregon\"),\n",
    "                (\"PA\",\"Pennsylvania\"),\n",
    "                (\"RI\",\"Rhode Island\"),\n",
    "                (\"SC\",\"South Carolina\"),\n",
    "                (\"SD\",\"South Dakota\"),\n",
    "                (\"TN\",\"Tennessee\"),\n",
    "                (\"TX\",\"Texas\"),\n",
    "                (\"UT\",\"Utah\"),\n",
    "                (\"VT\",\"Vermont\"),\n",
    "                (\"VA\",\"Virginia\"),\n",
    "                (\"WA\",\"Washington\"),\n",
    "                (\"WV\",\"West Virginia\"),\n",
    "                (\"WI\",\"Wisconsin\"),\n",
    "                (\"WY\",\"Wyoming\")]\n",
    "\n",
    "\n",
    "# Add sine and cos features for seasonal elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write clean data to 02_intermediate data folder\n",
    "weather_summary.to_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/03_processed/data_std_feature_eng.csv')\n",
    "\n",
    "# Save the features to a pickle file\n",
    "\n",
    "\n",
    "#weather_summary_tm.to_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/03_processed/data_tm_feature_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_summary['Snowfall_bin'].value_counts().sort_values().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call regplot on each axes\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "\n",
    "sns.displot(weather_summary, x=\"MIN\", ax=ax1)\n",
    "sns.displot(weather_summary, x=\"MAX\", ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(weather_summary, x=\"MAX\", hue=\"YR\", kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_summary[weather_summary['STA'] == 10001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_summary = pd.read_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/02_intermediate/data_cleaning.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Analysis & Visualization of features**\n",
    "- Timeseries Dataframe: weather_summary_tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort the data into date order and reset index\n",
    "## Create a new timeseries dataframe \n",
    "weather_summary.set_index('Date', drop=True, inplace=True)\n",
    "weather_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Analysis\n",
    "plt.scatter(weather_summary.DA, weather_summary.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TimeSeries Analysis \n",
    "# sns.lineplot(x='Date', \n",
    "#              y='MIN', \n",
    "#              data=weather_summarytrans, \n",
    "#              hue='STA'); # ';' is to avoid extra message before plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Location Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis & Expectations to Test**\n",
    "- What are the locations associated with the study?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_min_temperature = Data.loc[0, 'Average Tank Temperature (deg F)']\n",
    "initial_max_temperature = Data.loc[0, 'Average Tank Temperature (deg F)']\n",
    "\n",
    "final_min_temperature = Data.loc[Data.index.max(), 'Average Tank Temperature (deg F)']\n",
    "final_max_temperature = Data.loc[Data.index.max(), 'Average Tank Temperature (deg F)']\n",
    "\n",
    "min_temperature = Data['T_Amb (deg F)'].min()\n",
    "max_temperature = Data['T_Amb (deg F)'].max()\n",
    "\n",
    "min_temperature_sd = Data['T_Amb (deg F)'].sd()\n",
    "max_temperature_sd = Data['T_Amb (deg F)'].sd()\n",
    "\n",
    "min_temperature_avg = Data['T_Amb (deg F)'].mean()\n",
    "max_temperature_avg = Data['T_Amb (deg F)'].mean()\n",
    "\n",
    "min_temperature_median\n",
    "max_temperature_median\n",
    "\n",
    "min_temperature_mode\n",
    "max_temperature_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building \n",
    "- Data partitioning into training, validation and testing sets (set seed)\n",
    "    - Ensure that the range of the test is within the range of the train \n",
    "- Select the model that you would like to use\n",
    "- Hyperparameter tuning is used to fine-tune the model in order to prevent overfitting \n",
    "- Cross-validation is performed to ensure the model performs well on the validation set \n",
    "- Model is applied to the test data set\n",
    "- Save the trained model to a pickle file\n",
    "- Create a learning curve\n",
    "- Assess model bias and variance to deduce model improvement next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Remove date - this needs to have feature engineering applied\n",
    "combined_data.drop(['Date'], axis=1, inplace=True) \n",
    "\n",
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(combined_data, combined_data.MAX, test_size=0.3, random_state=0)\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution between the train and test dataset\n",
    "# Sweetviz notes: Only supports numeric & boolean targets currently\n",
    "my_report_train_test = sv.compare([x_train, \"Train\"], [x_test, \"Test\"], \"MAX\")\n",
    "my_report_train_test.show_html() # Not providing a filename will default to SWEETVIZ_REPORT.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SuspiciousTests_Test = pd.DataFrame(columns = ['Filename', 'Test Parameters', 'Code', 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
