{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Start Date** <br>\n",
    "17th December 2020 <br>\n",
    "\n",
    "**Data Sources** <br>\n",
    "https://www.kaggle.com/usaf/world-war-ii/notebooks <br> \n",
    "https://www.kaggle.com/smid80/weatherww2/data <br>\n",
    "\n",
    "**Background** <br>\n",
    "Aerial Bombing Operations in WW2 - Bombing operations data <br>\n",
    "\n",
    "This dataset consists of digitized paper mission reports from WWII. Each record includes the date, conflict, geographic location, and other data elements to form a live-action sequence of air warfare from 1939 to 1945. The records include U.S. and Royal Air Force data, in addition to some Australian, New Zealand and South African air force missions.\n",
    "\n",
    "Weather Conditions in WW2 (Weather Stations / Weather Conditions) <br>\n",
    "The dataset contains information on weather conditions recorded on each day at various weather stations around the world. Information includes precipitation, snowfall, temperatures, wind speed and whether the day included thunder storms or other poor weather conditions.\n",
    "\n",
    "**Aim of this project** <br>\n",
    "Implement a GLM model that predicts the maximum weather temperature (based on the minimum temperature)\n",
    "\n",
    "**Analysis regarding Data Quality** <br>\n",
    "Understanding of the sampling procedure \n",
    "- Since our project team did not participate in planning the study or data collection, it is possible that we are missing crucial context which could render our conclusions invalid. <br>\n",
    "\n",
    "Potential biases <br> \n",
    "Real-world actions that generated the data you inherited <br>\n",
    "\n",
    "**Objectives & Hypothesises to Test (max. 10)** <br>\n",
    "<u>Exploratory Analysis</u>\n",
    "- High level discriptive statistics \n",
    "- Do any values look to be recorded to accommodate missing values? e.g. 999, 9999 etc.\n",
    "- Assessment of feature distributions\n",
    "    https://towardsdatascience.com/ridgeline-plots-the-perfect-way-to-visualize-data-distributions-with-python-de99a5493052\n",
    "- Assessment of feature relationships:\n",
    "    - Is there a relationship between the daily minimum and maximum temperature (TimeSeries Analysis)?\n",
    "    - It is expected that average temperatures are colder in winter months than summer months\n",
    "    - It is expected that more snowfall occurs in the winter months (for northern hemisphere regions)\n",
    "    - It is expected that more Precipitation occurs in the winter months (for northern hemisphere)\n",
    "    - It is expected that lower temperatures correlate with higher snowfall and precipation \n",
    "    - It is expected that higher levels above the sea have greater precipation\n",
    "    - It is expected that the accuracy of recordings based on stations may not be uniform (outlier detection)\n",
    "<br>\n",
    "\n",
    "**Statistical Model/Machine Learning Applications**\n",
    "- Create a dummy model (Predict the average temperature for that monthly/quarter)\n",
    "- Explain the train/test split\n",
    "- Predict the maximum temperature given the minimum temperature (GLM Models & Bayesian Versions)?\n",
    "- Explain appropriate error metric\n",
    "- Explain class balance and any required action\n",
    "- Explain what features are developed and transformations applied\n",
    "- Explain if the model is exhibiting high bias or high variance and how this can be improved\n",
    "    - Plot learning curves to deduce high bias/high variance and conclude what means could be applied to solve these issues\n",
    "- Explain where the model seems to perform poorly - In what situations does the model make mistakes?\n",
    "\n",
    "**Additional Learning notes from Reviewing 3 other Kaggle Notebooks** <br> \n",
    "\n",
    "**Next steps** <br>\n",
    "\n",
    "**References**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Requirements\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "\n",
    "# Data Exploration and Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aerial_bombing_data = pd.read_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/01_raw/ww2_boming_operations.csv')\n",
    "weather_summary = pd.read_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/01_raw/WeatherTempPrediction.csv')\n",
    "weather_station_location = pd.read_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/01_raw/WeatherStationLocations.csv')\n",
    "\n",
    "data_list = []\n",
    "data_list.append(aerial_bombing_data)\n",
    "data_list.append(weather_summary)\n",
    "data_list.append(weather_station_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State the assumptions youâ€™re being forced to make.\n",
    "# Write up caveat notes to be included in the appendix of your final report\n",
    "# Write cautionary notes that warn the decision-maker (and your other readers) that conclusions from the study will \n",
    "# need to be downgraded due to potential data issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_list:\n",
    "    print(\"Dataframe Dimensions\")\n",
    "    print(i.shape)\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Dataframe Columns and respective types\")\n",
    "    print(i.dtypes)\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Percentage of Missing Data\")\n",
    "    print(i.isnull().sum() * 100 / len(i))\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "- Data visualization of features\n",
    "- Handling categorical data\n",
    "- Normalization and standardization of features\n",
    "- Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investiage options to link the dataframes with a unique key: weather_summary and weather_station_location look to be \n",
    "# connected via STA and WBAN respectively \n",
    "def uncommon_elements(list1, list2):\n",
    "    ## Add something clever so the look up is always against the set with the largest number of unique records\n",
    "    \n",
    "    return [element for element in list2 if element not in list1]\n",
    "\n",
    "STA = set(weather_summary.STA)\n",
    "print(len(STA))\n",
    "\n",
    "WBAN = set(weather_station_location.WBAN)\n",
    "print(len(WBAN))\n",
    "\n",
    "print('Sets are of the same data type: ', type(weather_summary.STA) == type(weather_station_location.WBAN))\n",
    "\n",
    "print('Stations that are uncommon across both sets: ', uncommon_elements(STA, WBAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.merge(weather_summary, \n",
    "                         weather_station_location, \n",
    "                         how = 'inner', # takes care of only keeping records in both sets\n",
    "                         left_on='STA',\n",
    "                         right_on='WBAN')\n",
    "\n",
    "print(len(combined_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns for Combined Data\n",
    "**STA**\n",
    "- STA: represent the Weather Station\n",
    "- Not all STA codes represent the same time frequency \n",
    "\n",
    "**Date** \n",
    "- Date has been split into DA MO and YR respectively, note the century has been dropped when recording the YR\n",
    "\n",
    "**Precip** \n",
    "- Precipitation in mm. This consists of numerical values and 'T' for 16,754 entries. This looks to be a mistake in the data collection (Impute precip == 0 for these cases)\n",
    "\n",
    "**MaxTemp and MinTemp** \n",
    "- These are features that have been transformed into celcius from fahrenheit readings MAX/MIN and these have been recorded to 6 decimal places. The degrees celcius value has additionally been converted to an average. Using celcius will have a smaller range than the fahrenheit records. Patterns may be more easily seen based fahrenheit columns \n",
    "\n",
    "**MEA** \n",
    "- This is the mean for the fahrenheit MAX / MIN columns and this has been rounded to 1 d.p. Drop this columns and calculate the extact mean value\n",
    "\n",
    "**Snowfall**\n",
    "- This looks to be measures in terms of the amount of snow that fell in mm. The units are not obvious so there are two options\n",
    "- Either assume the units are centiments by attempting to research more about the data OR normalise all the columns so they are on the same scale\n",
    "\n",
    "**SNF**\n",
    " - After research it is unclear what SNF relates too and seems to gave a range of 0 - 3.4 (Agree to remove)\n",
    "\n",
    "**PRCP**\n",
    "- This column looks to have been scaled by a factor of 1/25.4*Precip (Agree to remove)\n",
    "\n",
    "**TSHDSBRSGF**\n",
    "- This is a repeat for PoorWeather so can be removed\n",
    "\n",
    "**WBAN**\n",
    "- Same as STA, representing the Weather Station\n",
    "- Not all Weather Stations are located in the USA (unique STATE/COUNTRY ID = 63)\n",
    "- This will be duplicated due to the merge so can be removed \n",
    "\n",
    "**NAME**\n",
    "- This is the name of the weather station. It has a many:1 relationship with State/Country ID i.e. more than one station can be present per country \n",
    "\n",
    "**STATE/COUNTRY ID**\n",
    "- This is the location of the weather station at state/country level\n",
    "\n",
    "**LAT**\n",
    "- This is the decimal latitude in string format \n",
    "\n",
    "**LON**\n",
    "- This is the decimal latitude in string format \n",
    "\n",
    "**ELEV**\n",
    "- Explanation not given - Expected to be level above the sea \n",
    "- Note that an elevation of 9999 means unknown\n",
    "\n",
    "**Latitude**\n",
    "- This is the decimal latitude calculated from the LAT/LON provided (use this over string as in format for ML)\n",
    "\n",
    "**Longitude**\n",
    "- This is the decimal longitude calculated from the LAT/LON provided (use this over string as in format for ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - Remove duplicate Rows & Columns\n",
    "- Remove all columns that exhibit over 90% missing values\n",
    "- Remove celcius columns 'MaxTemp', 'MinTemp', 'MeanTemp' and 'MEA'\n",
    "- Remove duplicated/scaled columns: 'PRCP', 'TSHDSBRSGF'\n",
    "- Remove PoorWeather for the inital analysis as unclear how the data has been recorded \n",
    "- Remove LAT as string format\n",
    "- Remove LON as string format\n",
    "- Remove those columns with zero variance\n",
    "- Remove duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing data - Remove any columns with over 90% missing data \n",
    "def remove_missing_values(data, thresold_limit = 0.9):\n",
    "    \n",
    "    return data.loc[:, data.isnull().sum() < thresold_limit*data.shape[0]]\n",
    "\n",
    "combined_data = remove_missing_values(combined_data)\n",
    "\n",
    "# Remove additional columns based on explanation above\n",
    "combined_data.drop(['MaxTemp', \n",
    "                    'MinTemp', \n",
    "                    'MeanTemp', \n",
    "                    'MEA', \n",
    "                    'PoorWeather', \n",
    "                    'TSHDSBRSGF', \n",
    "                    'PRCP', \n",
    "                    'SNF', \n",
    "                    'WBAN',\n",
    "                    'LAT', \n",
    "                    'LON'], axis=1, inplace=True)\n",
    "\n",
    "# Data Quality Expectations: Test for zero variance \n",
    "combined_data = combined_data.loc[:, combined_data.apply(pd.Series.nunique) != 1]\n",
    "\n",
    "# Data Quality Expectations: Duplicated Records\n",
    "print('Duplicated rows for index: ', combined_data[combined_data.duplicated()].index)\n",
    "#print(len(combined_data))\n",
    "combined_data = combined_data.drop_duplicates()\n",
    "#print(len(combined_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - Correct Data Types and Imputation of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Date\n",
    "weather_summary.Date = pd.to_datetime(weather_summary['Date'], format = '%Y-%m-%d')\n",
    "\n",
    "# Correct Object Types\n",
    "weather_summary.STA = weather_summary['STA'].astype('object')\n",
    "weather_summary.YR = weather_summary['YR'].astype('object')\n",
    "weather_summary.MO = weather_summary['MO'].astype('object')\n",
    "weather_summary.DA = weather_summary['DA'].astype('object')\n",
    "\n",
    "# Deal with Missing/Inaccurate values and correct data types \n",
    "weather_summary.Precip = np.where((weather_summary.Precip == 'T') | (weather_summary.Precip == ' '), 0, weather_summary.Precip)\n",
    "weather_summary.Precip = weather_summary['Precip'].astype('float')\n",
    "\n",
    "weather_summary.SNF = np.where((weather_summary.SNF == 'T') | (weather_summary.SNF == ' '), 0, weather_summary.SNF)\n",
    "weather_summary.SNF = weather_summary['SNF'].astype('float')\n",
    "\n",
    "# Check if any features are transformations of each other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis \n",
    "**Hypothesis & Expectations to Test**\n",
    "- Is this a Global study? What are the locations associated with the experiment?\n",
    "- High level discriptive statistics \n",
    "- Do any values look to be recorded to accommodate missing values? e.g. 999, 9999 etc.\n",
    "- Assessment of feature distributions\n",
    "- Assessment of feature relationships:\n",
    "    - It is expected that average temperatures are colder in winter months than summer months\n",
    "    - It is expected that more snowfall occurs in the winter months (for northern hemisphere regions)\n",
    "    - It is expected that more Precipitation occurs in the winter months (for northern hemisphere)\n",
    "    - It is expected that lower temperatures correlate with higher snowfall and precipation \n",
    "    - It is expected that higher levels above the sea have greater precipation\n",
    "    - It is expected that the accuracy of recordings based on stations may not be uniform (outlier detection)\n",
    "- Time Series Analysis (max. 3 graphs/analyses)\n",
    "\n",
    "**References** <br>\n",
    "https://towardsdatascience.com/powerful-eda-exploratory-data-analysis-in-just-two-lines-of-code-using-sweetviz-6c943d32f34 <br>\n",
    "https://towardsdatascience.com/ridgeline-plots-the-perfect-way-to-visualize-data-distributions-with-python-de99a5493052 <br>\n",
    "https://towardsdatascience.com/all-you-want-to-know-about-preprocessing-data-preparation-b6c2866071d4 <br>\n",
    "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(combined_data['NAME'], combined_data['STATE/COUNTRY ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write clean data to 02_intermediate data folder\n",
    "#weather_summary.to_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/02_intermediate/data_cleaning.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_summary = pd.read_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/02_intermediate/data_cleaning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average based on the fahrenheit columns\n",
    "weather_summary['MeanTemp_F'] = (weather_summary['MAX'] + weather_summary['MIN'])/2\n",
    "\n",
    "# Create a simplier binary feature for snowfall occurrence\n",
    "weather_summary['Snowfall_bin'] =  np.where(weather_summary.Snowfall.isnull(), 0, 1)\n",
    "\n",
    "# Add full state names to the analysis\n",
    "\n",
    "STATES_TUPLE = [(\"AL\",\"Alabama\"),\n",
    "                (\"AK\",\"Alaska\"),\n",
    "                (\"AZ\",\"Arizona\"),\n",
    "                (\"AR\",\"Arkansas\"),\n",
    "                (\"CA\", \"California\"),\n",
    "                (\"CO\", \"Colorado\"),\n",
    "                (\"CT\",\"Connecticut\"),\n",
    "                (\"DC\",\"Washington DC\"),\n",
    "                (\"DE\",\"Delaware\"),\n",
    "                (\"FL\",\"Florida\"),\n",
    "                (\"GA\",\"Georgia\"),\n",
    "                (\"HI\",\"Hawaii\"),\n",
    "                (\"ID\",\"Idaho\"),\n",
    "                (\"IL\",\"Illinois\"),\n",
    "                (\"IN\",\"Indiana\"),\n",
    "                (\"IA\",\"Iowa\"),\n",
    "                (\"KS\",\"Kansas\"),\n",
    "                (\"KY\",\"Kentucky\"),\n",
    "                (\"LA\",\"Louisiana\"),\n",
    "                (\"ME\",\"Maine\"),\n",
    "                (\"MD\",\"Maryland\"),\n",
    "                (\"MA\",\"Massachusetts\"),\n",
    "                (\"MI\",\"Michigan\"),\n",
    "                (\"MN\",\"Minnesota\"),\n",
    "                (\"MS\",\"Mississippi\"),\n",
    "                (\"MO\",\"Missouri\"),\n",
    "                (\"MT\",\"Montana\"),\n",
    "                (\"NE\",\"Nebraska\"),\n",
    "                (\"NV\",\"Nevada\"),\n",
    "                (\"NH\",\"New Hampshire\"),\n",
    "                (\"NJ\",\"New Jersey\"),\n",
    "                (\"NM\",\"New Mexico\"),\n",
    "                (\"NY\",\"New York\"),\n",
    "                (\"NC\",\"North Carolina\"),\n",
    "                (\"ND\",\"North Dakota\"),\n",
    "                (\"OH\",\"Ohio\"),\n",
    "                (\"OK\",\"Oklahoma\"),\n",
    "                (\"OR\",\"Oregon\"),\n",
    "                (\"PA\",\"Pennsylvania\"),\n",
    "                (\"RI\",\"Rhode Island\"),\n",
    "                (\"SC\",\"South Carolina\"),\n",
    "                (\"SD\",\"South Dakota\"),\n",
    "                (\"TN\",\"Tennessee\"),\n",
    "                (\"TX\",\"Texas\"),\n",
    "                (\"UT\",\"Utah\"),\n",
    "                (\"VT\",\"Vermont\"),\n",
    "                (\"VA\",\"Virginia\"),\n",
    "                (\"WA\",\"Washington\"),\n",
    "                (\"WV\",\"West Virginia\"),\n",
    "                (\"WI\",\"Wisconsin\"),\n",
    "                (\"WY\",\"Wyoming\")]\n",
    "\n",
    "\n",
    "# Add sine and cos features for seasonal elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write clean data to 02_intermediate data folder\n",
    "weather_summary.to_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/03_processed/data_std_feature_eng.csv')\n",
    "\n",
    "# Save the features to a pickle file\n",
    "\n",
    "\n",
    "#weather_summary_tm.to_csv('/Users/Rej1992/Documents/GitHub/RegressionModels/data/03_processed/data_tm_feature_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_summary['Snowfall_bin'].value_counts().sort_values().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call regplot on each axes\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "\n",
    "sns.displot(weather_summary, x=\"MIN\", ax=ax1)\n",
    "sns.displot(weather_summary, x=\"MAX\", ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(weather_summary, x=\"MAX\", hue=\"YR\", kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_summary[weather_summary['STA'] == 10001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Analysis & Visualization of features**\n",
    "- Timeseries Dataframe: weather_summary_tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort the data into date order and reset index\n",
    "## Create a new timeseries dataframe \n",
    "weather_summary.set_index('Date', drop=True, inplace=True)\n",
    "weather_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Analysis\n",
    "plt.scatter(weather_summary.DA, weather_summary.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TimeSeries Analysis \n",
    "# sns.lineplot(x='Date', \n",
    "#              y='MIN', \n",
    "#              data=weather_summarytrans, \n",
    "#              hue='STA'); # ';' is to avoid extra message before plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Location Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis & Expectations to Test**\n",
    "- What are the locations associated with the study?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_min_temperature = Data.loc[0, 'Average Tank Temperature (deg F)']\n",
    "initial_max_temperature = Data.loc[0, 'Average Tank Temperature (deg F)']\n",
    "\n",
    "final_min_temperature = Data.loc[Data.index.max(), 'Average Tank Temperature (deg F)']\n",
    "final_max_temperature = Data.loc[Data.index.max(), 'Average Tank Temperature (deg F)']\n",
    "\n",
    "min_temperature = Data['T_Amb (deg F)'].min()\n",
    "max_temperature = Data['T_Amb (deg F)'].max()\n",
    "\n",
    "min_temperature_sd = Data['T_Amb (deg F)'].sd()\n",
    "max_temperature_sd = Data['T_Amb (deg F)'].sd()\n",
    "\n",
    "min_temperature_avg = Data['T_Amb (deg F)'].mean()\n",
    "max_temperature_avg = Data['T_Amb (deg F)'].mean()\n",
    "\n",
    "min_temperature_median\n",
    "max_temperature_median\n",
    "\n",
    "min_temperature_mode\n",
    "max_temperature_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building \n",
    "- Data partitioning into training, validation and testing sets\n",
    "- Select the model that you would like to use\n",
    "- Hyperparameter tuning is used to fine-tune the model in order to prevent overfitting \n",
    "- Cross-validation is performed to ensure the model performs well on the validation set \n",
    "- Model is applied to the test data set\n",
    "- Save the trained model to a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SuspiciousTests_Test = pd.DataFrame(columns = ['Filename', 'Test Parameters', 'Code', 'Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
